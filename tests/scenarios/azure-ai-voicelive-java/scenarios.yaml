# yaml-language-server: $schema=../skill-scenarios.schema.json
# Test scenarios for azure-ai-voicelive-java skill evaluation

config:
  model: gpt-4
  max_tokens: 2000
  temperature: 0.3

scenarios:
  - name: create_voicelive_client_api_key
    prompt: |
      Create a VoiceLiveAsyncClient using API key authentication.
      Use environment variables for endpoint and key.
    expected_patterns:
      - "VoiceLiveClientBuilder"
      - "buildAsyncClient"
      - "AzureKeyCredential"
      - "System.getenv"
    forbidden_patterns:
      - "new VoiceLiveAsyncClient("
      - "buildClient"
    tags:
      - client-creation
      - authentication
    mock_response: |
      import com.azure.ai.voicelive.VoiceLiveAsyncClient;
      import com.azure.ai.voicelive.VoiceLiveClientBuilder;
      import com.azure.core.credential.AzureKeyCredential;

      VoiceLiveAsyncClient client = new VoiceLiveClientBuilder()
          .endpoint(System.getenv("AZURE_VOICELIVE_ENDPOINT"))
          .credential(new AzureKeyCredential(System.getenv("AZURE_VOICELIVE_API_KEY")))
          .buildAsyncClient();

  - name: create_voicelive_client_default_credential
    prompt: |
      Create a VoiceLiveAsyncClient using DefaultAzureCredential for managed identity.
    expected_patterns:
      - "VoiceLiveClientBuilder"
      - "buildAsyncClient"
      - "DefaultAzureCredentialBuilder"
      - ".build()"
    forbidden_patterns:
      - "AzureKeyCredential"
      - "new DefaultAzureCredential()"
    tags:
      - client-creation
      - managed-identity
    mock_response: |
      import com.azure.ai.voicelive.VoiceLiveAsyncClient;
      import com.azure.ai.voicelive.VoiceLiveClientBuilder;
      import com.azure.identity.DefaultAzureCredentialBuilder;

      VoiceLiveAsyncClient client = new VoiceLiveClientBuilder()
          .endpoint(System.getenv("AZURE_VOICELIVE_ENDPOINT"))
          .credential(new DefaultAzureCredentialBuilder().build())
          .buildAsyncClient();

  - name: start_voice_session
    prompt: |
      Start a voice session with the gpt-4o-realtime-preview model and subscribe to events.
    expected_patterns:
      - "startSession"
      - "gpt-4o-realtime-preview"
      - "flatMap"
      - "receiveEvents"
      - "subscribe"
      - "Mono"
    forbidden_patterns:
      - "buildClient"
    tags:
      - session-management
    mock_response: |
      import reactor.core.publisher.Mono;

      client.startSession("gpt-4o-realtime-preview")
          .flatMap(session -> {
              System.out.println("Session started");
              
              session.receiveEvents()
                  .subscribe(
                      event -> System.out.println("Event: " + event.getType()),
                      error -> System.err.println("Error: " + error.getMessage())
                  );
              
              return Mono.just(session);
          })
          .block();

  - name: configure_session_options
    prompt: |
      Configure a voice session with turn detection, transcription, and voice settings.
    expected_patterns:
      - "VoiceLiveSessionOptions"
      - "ServerVadTurnDetection"
      - "AudioInputTranscriptionOptions"
      - "setInstructions"
      - "setVoice"
      - "setModalities"
      - "setTurnDetection"
      - "ClientEventSessionUpdate"
    forbidden_patterns:
      - "VoiceLiveClient"
    tags:
      - session-configuration
    mock_response: |
      import com.azure.ai.voicelive.models.*;
      import com.azure.core.util.BinaryData;
      import java.util.Arrays;

      ServerVadTurnDetection turnDetection = new ServerVadTurnDetection()
          .setThreshold(0.5)
          .setPrefixPaddingMs(300)
          .setSilenceDurationMs(500)
          .setInterruptResponse(true)
          .setAutoTruncate(true)
          .setCreateResponse(true);

      AudioInputTranscriptionOptions transcription = new AudioInputTranscriptionOptions(
          AudioInputTranscriptionOptionsModel.WHISPER_1);

      VoiceLiveSessionOptions options = new VoiceLiveSessionOptions()
          .setInstructions("You are a helpful AI voice assistant.")
          .setVoice(BinaryData.fromObject(new OpenAIVoice(OpenAIVoiceName.ALLOY)))
          .setModalities(Arrays.asList(InteractionModality.TEXT, InteractionModality.AUDIO))
          .setInputAudioFormat(InputAudioFormat.PCM16)
          .setOutputAudioFormat(OutputAudioFormat.PCM16)
          .setInputAudioSamplingRate(24000)
          .setInputAudioTranscription(transcription)
          .setTurnDetection(turnDetection);

      ClientEventSessionUpdate updateEvent = new ClientEventSessionUpdate(options);
      session.sendEvent(updateEvent).subscribe();

  - name: send_audio_input
    prompt: |
      Send audio input data to the voice session.
    expected_patterns:
      - "sendInputAudio"
      - "BinaryData.fromBytes"
      - "subscribe"
    forbidden_patterns:
      - "sendAudio"
    tags:
      - audio-input
    mock_response: |
      import com.azure.core.util.BinaryData;

      byte[] audioData = readAudioChunk(); // Your PCM16 audio data at 24kHz
      session.sendInputAudio(BinaryData.fromBytes(audioData)).subscribe();

  - name: handle_events
    prompt: |
      Handle voice session events including session created, speech started/stopped, 
      audio response, and errors.
    expected_patterns:
      - "receiveEvents"
      - "subscribe"
      - "ServerEventType"
      - "SESSION_CREATED"
      - "INPUT_AUDIO_BUFFER_SPEECH_STARTED"
      - "INPUT_AUDIO_BUFFER_SPEECH_STOPPED"
      - "RESPONSE_AUDIO_DELTA"
      - "RESPONSE_DONE"
      - "ERROR"
    forbidden_patterns:
      - "try.*catch"
    tags:
      - event-handling
    mock_response: |
      import com.azure.ai.voicelive.models.*;

      session.receiveEvents().subscribe(event -> {
          ServerEventType eventType = event.getType();
          
          if (ServerEventType.SESSION_CREATED.equals(eventType)) {
              System.out.println("Session created");
          } else if (ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STARTED.equals(eventType)) {
              System.out.println("User started speaking");
          } else if (ServerEventType.INPUT_AUDIO_BUFFER_SPEECH_STOPPED.equals(eventType)) {
              System.out.println("User stopped speaking");
          } else if (ServerEventType.RESPONSE_AUDIO_DELTA.equals(eventType)) {
              if (event instanceof SessionUpdateResponseAudioDelta) {
                  SessionUpdateResponseAudioDelta audioEvent = (SessionUpdateResponseAudioDelta) event;
                  playAudioChunk(audioEvent.getDelta());
              }
          } else if (ServerEventType.RESPONSE_DONE.equals(eventType)) {
              System.out.println("Response complete");
          } else if (ServerEventType.ERROR.equals(eventType)) {
              if (event instanceof SessionUpdateError) {
                  SessionUpdateError errorEvent = (SessionUpdateError) event;
                  System.err.println("Error: " + errorEvent.getError().getMessage());
              }
          }
      });

  - name: configure_openai_voice
    prompt: |
      Configure the session to use an OpenAI voice (Alloy).
    expected_patterns:
      - "OpenAIVoice"
      - "OpenAIVoiceName.ALLOY"
      - "BinaryData.fromObject"
      - "setVoice"
    forbidden_patterns:
      - "AzureStandardVoice"
    tags:
      - voice-configuration
    mock_response: |
      import com.azure.ai.voicelive.models.*;
      import com.azure.core.util.BinaryData;

      // Available: ALLOY, ASH, BALLAD, CORAL, ECHO, SAGE, SHIMMER, VERSE
      VoiceLiveSessionOptions options = new VoiceLiveSessionOptions()
          .setVoice(BinaryData.fromObject(new OpenAIVoice(OpenAIVoiceName.ALLOY)));

  - name: configure_azure_voice
    prompt: |
      Configure the session to use an Azure standard voice.
    expected_patterns:
      - "AzureStandardVoice"
      - "BinaryData.fromObject"
      - "setVoice"
    forbidden_patterns:
      - "OpenAIVoice"
    tags:
      - voice-configuration
    mock_response: |
      import com.azure.ai.voicelive.models.*;
      import com.azure.core.util.BinaryData;

      // Azure Standard Voice
      VoiceLiveSessionOptions options = new VoiceLiveSessionOptions()
          .setVoice(BinaryData.fromObject(new AzureStandardVoice("en-US-JennyNeural")));

  - name: function_calling
    prompt: |
      Configure a voice session with function calling capabilities for weather information.
    expected_patterns:
      - "VoiceLiveFunctionDefinition"
      - "setDescription"
      - "setParameters"
      - "setTools"
      - "BinaryData.fromObject"
    forbidden_patterns:
      - "OpenAIVoice"
    tags:
      - function-calling
    mock_response: |
      import com.azure.ai.voicelive.models.*;
      import com.azure.core.util.BinaryData;
      import java.util.Arrays;

      VoiceLiveFunctionDefinition weatherFunction = new VoiceLiveFunctionDefinition("get_weather")
          .setDescription("Get current weather for a location")
          .setParameters(BinaryData.fromObject(parametersSchema));

      VoiceLiveSessionOptions options = new VoiceLiveSessionOptions()
          .setTools(Arrays.asList(weatherFunction))
          .setInstructions("You have access to weather information.");

  - name: error_handling_reactive
    prompt: |
      Implement proper error handling for voice session events using reactive patterns.
    expected_patterns:
      - "doOnError"
      - "onErrorResume"
      - "Flux.empty"
      - "getMessage"
    forbidden_patterns:
      - "try"
      - "catch"
    tags:
      - error-handling
    mock_response: |
      import reactor.core.publisher.Flux;

      session.receiveEvents()
          .doOnError(error -> System.err.println("Connection error: " + error.getMessage()))
          .onErrorResume(error -> {
              // Attempt reconnection or cleanup
              return Flux.empty();
          })
          .subscribe();

  - name: configure_audio_processing
    prompt: |
      Configure audio input with noise reduction and echo cancellation.
    expected_patterns:
      - "setInputAudioNoiseReduction"
      - "AudioNoiseReduction"
      - "AudioNoiseReductionType"
      - "setInputAudioEchoCancellation"
      - "AudioEchoCancellation"
    forbidden_patterns:
      - "setVoice"
    tags:
      - audio-processing
    mock_response: |
      import com.azure.ai.voicelive.models.*;

      VoiceLiveSessionOptions options = new VoiceLiveSessionOptions()
          .setInputAudioFormat(InputAudioFormat.PCM16)
          .setOutputAudioFormat(OutputAudioFormat.PCM16)
          .setInputAudioSamplingRate(24000)
          .setInputAudioNoiseReduction(new AudioNoiseReduction(AudioNoiseReductionType.NEAR_FIELD))
          .setInputAudioEchoCancellation(new AudioEchoCancellation());
